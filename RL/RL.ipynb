{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m|\u001b[43m \u001b[0m: |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym \n",
    "from gym import envs\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('Taxi-v2')\n",
    "env.reset()\n",
    "env.render()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m|\u001b[43m \u001b[0m: |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m|\u001b[43m \u001b[0m: |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m|\u001b[43m \u001b[0m: |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m|\u001b[43m \u001b[0m: |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "-22\n"
     ]
    }
   ],
   "source": [
    "totReward = 0\n",
    "env.render()\n",
    "for i in range(4):\n",
    "    action = env.action_space.sample() #take step using random action from possible actions (actio_space)\n",
    "    obs, rew, done, info = env.step(action) \n",
    "    totReward += rew\n",
    "    env.render()\n",
    "print(totReward) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "6\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: |\u001b[43m \u001b[0m: :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space.n) #0...499\n",
    "print(env.action_space.n) #0..5\n",
    "env.env.s=42 # some random number, you might recognize it\n",
    "env.render()\n",
    "env.env.s = 0 # and some other\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##value iteration\n",
    "\n",
    "env.reset()\n",
    "NUM_ACTIONS = env.action_space.n\n",
    "NUM_STATES = env.observation_space.n\n",
    "V_old = np.zeros(NUM_STATES)\n",
    "V = np.zeros(NUM_STATES)\n",
    "Pi = np.zeros(NUM_STATES,dtype=int) #deterministisk policy som vi hittar i slutet. För varje state säger vilken action\n",
    "iterations = 500\n",
    "discount = 0.8\n",
    "\n",
    "def findV(state):\n",
    "    bestValue = -1000000\n",
    "    for action in range(NUM_ACTIONS):\n",
    "        env.env.s=state\n",
    "        new_state, rew, done, info = env.step(action)\n",
    "        if rew+discount*V_old[new_state] > bestValue:\n",
    "            bestValue = rew+discount*V_old[new_state]\n",
    "    return bestValue\n",
    "\n",
    "for i in range(iterations):\n",
    "    for s in range(NUM_STATES):\n",
    "        V[s] = findV(s)\n",
    "    V_old = V\n",
    "    \n",
    "for state in range(NUM_STATES):\n",
    "    bestAction = None\n",
    "    bestValue = -1000000\n",
    "    for action in range(NUM_ACTIONS):\n",
    "        env.env.s = state\n",
    "        new_state, rew, done, info = env.step(action)\n",
    "        if rew+discount*V[new_state] > bestValue:\n",
    "            bestValue = rew+discount*V[new_state]\n",
    "            bestAction=action\n",
    "    Pi[state] = bestAction\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m:\u001b[43m \u001b[0m| : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|\u001b[42mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "|\u001b[42m_\u001b[0m: : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "|\u001b[42m_\u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "|\u001b[42m_\u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[42mY\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "done=False\n",
    "totReward = 0\n",
    "while not done:\n",
    "    state, rew, done, info = env.step(Pi[state])\n",
    "    totReward += rew\n",
    "    env.render()\n",
    "print(totReward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-686\n",
      "9\n",
      "7\n",
      "11\n",
      "7\n",
      "8\n",
      "12\n",
      "7\n",
      "12\n",
      "8\n",
      "8\n",
      "6\n",
      "7\n",
      "7\n",
      "11\n",
      "9\n",
      "11\n",
      "7\n",
      "7\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "env.reset() # env = gym.make('Taxi_driver_v2.0')\n",
    "NUM_ACTIONS = env.action_space.n\n",
    "NUM_STATES = env.observation_space.n\n",
    "\n",
    "Q = np.zeros((NUM_STATES,NUM_ACTIONS))\n",
    "alpha = 0.7\n",
    "gamma = 0.7\n",
    "episodes = 10000\n",
    "eps = 1.0\n",
    "\n",
    "for i in range(episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    tot_reward = 0\n",
    "    while not done:\n",
    "        if np.random.rand(1) < 1-eps:\n",
    "            action = np.argmax(Q[state,:])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        new_state, rew, done, info = env.step(action)\n",
    "        Q[state,action] += alpha*(rew+gamma*np.max(Q[new_state,:])-Q[state,action])\n",
    "        tot_reward += rew\n",
    "        state = new_state\n",
    "    eps = eps*0.99\n",
    "    if i % 500 == 0:\n",
    "        print(tot_reward)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : : |\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : :\u001b[43m \u001b[0m: |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[42mB\u001b[0m: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[42m_\u001b[0m: |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[42m_\u001b[0m|\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : :\u001b[42m_\u001b[0m|\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : :\u001b[42m_\u001b[0m|\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35m\u001b[42mG\u001b[0m\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[35m\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "tot_reward = 0\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = np.argmax(Q[state,:])\n",
    "    state, rew, done, info = env.step(action)\n",
    "    tot_reward += rew\n",
    "    env.render()\n",
    "print(rew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-542\n",
      "8\n",
      "7\n",
      "12\n",
      "7\n",
      "11\n",
      "8\n",
      "8\n",
      "7\n",
      "7\n",
      "13\n",
      "10\n",
      "6\n",
      "7\n",
      "6\n",
      "5\n",
      "9\n",
      "8\n",
      "6\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "## Q learning\n",
    "\n",
    "env.reset()\n",
    "NUM_ACTIONS = env.action_space.n\n",
    "NUM_STATES = env.observation_space.n\n",
    "Q = np.zeros((NUM_STATES,NUM_ACTIONS))\n",
    "episodes = 10000\n",
    "discount = 0.8\n",
    "alpha = 0.7\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    totReward = 0\n",
    "    while not done:\n",
    "        action = np.argmax(Q[state]) #note dont do epsilon greedy here\n",
    "        new_state, rew, done, info = env.step(action)\n",
    "        Q[state,action] += alpha*(rew+np.max(Q[new_state])-Q[state,action])\n",
    "        totReward += rew\n",
    "        state = new_state\n",
    "    if (episode % 500 == 0):\n",
    "        print(totReward)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "done=False\n",
    "totReward = 0\n",
    "while not done:\n",
    "    state, rew, done, info = env.step(np.argmax(Q[state]))\n",
    "    totReward += rew\n",
    "    env.render()\n",
    "print(totReward)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Q-learning with Frozen lake. Now dont have deterministic anymore. We dont know transition probabilities\n",
    "env = gym.make('FrozenLake-v0')  ## -1 reward every action and (so solve fast) -10 fall in hole\n",
    "env.reset()                      ## and 30 if hit goal\n",
    "NUM_ACTIONS = env.action_space.n\n",
    "NUM_STATES = env.observation_space.n\n",
    "discount = 0.9\n",
    "alpha = 0.01\n",
    "epsilon = 0.8\n",
    "episodes = 80000\n",
    "Q = np.zeros((NUM_STATES,NUM_ACTIONS))\n",
    "\n",
    "for episode in range(episodes):\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    tot_reward = 0\n",
    "    while not done:\n",
    "        if np.random.rand(1) < epsilon: ## epsilon-greedy: med prob eps välja för att utforska annars ta säkra\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(Q[state])\n",
    "        new_state, rew, done, info = env.step(action)\n",
    "        Q[state,action] += alpha*(rew+np.max(Q[new_state])-Q[state,action])\n",
    "        state = new_state\n",
    "        tot_reward += rew\n",
    "        print(state)\n",
    "    eps *= 0.99\n",
    "    if episode % 500 == 0:\n",
    "        print(tot_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "done=False\n",
    "totReward = 0\n",
    "while not done:\n",
    "    state, rew, done, info = env.step(np.argmax(Q[state]))\n",
    "    totReward += rew\n",
    "    env.render()\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "23.0\n",
      "0.995\n",
      "12.0\n",
      "0.9275689688183278\n",
      "41.0\n",
      "0.7477194593032545\n",
      "23.0\n",
      "0.6596532430440636\n",
      "47.0\n",
      "0.5159963842937159\n",
      "19.0\n",
      "0.46444185833082485\n",
      "11.0\n",
      "0.4351424010585501\n",
      "58.0\n",
      "0.322118930542046\n",
      "121.0\n",
      "0.17388222158237718\n",
      "43.0\n",
      "0.138769433003975\n",
      "273.0\n",
      "0.03496560272750046\n",
      "238.0\n",
      "0.010499784796482848\n",
      "498.0\n",
      "0.01\n",
      "369.0\n",
      "0.01\n",
      "498.0\n",
      "0.01\n",
      "274.0\n",
      "0.01\n",
      "328.0\n",
      "0.01\n",
      "185.0\n",
      "0.01\n",
      "186.0\n",
      "0.01\n",
      "477.0\n",
      "0.01\n",
      "252.0\n",
      "0.01\n",
      "271.0\n",
      "0.01\n",
      "194.0\n",
      "0.01\n",
      "263.0\n",
      "0.01\n",
      "212.0\n",
      "0.01\n",
      "245.0\n",
      "0.01\n",
      "278.0\n",
      "0.01\n",
      "382.0\n",
      "0.01\n",
      "181.0\n",
      "0.01\n",
      "325.0\n",
      "0.01\n",
      "148.0\n",
      "0.01\n",
      "138.0\n",
      "0.01\n",
      "319.0\n",
      "0.01\n",
      "451.0\n",
      "0.01\n",
      "273.0\n",
      "0.01\n",
      "498.0\n",
      "0.01\n",
      "347.0\n",
      "0.01\n",
      "498.0\n",
      "0.01\n",
      "224.0\n",
      "0.01\n",
      "124.0\n",
      "0.01\n",
      "330.0\n",
      "0.01\n",
      "343.0\n",
      "0.01\n",
      "181.0\n",
      "0.01\n",
      "134.0\n",
      "0.01\n",
      "173.0\n",
      "0.01\n",
      "219.0\n",
      "0.01\n",
      "351.0\n",
      "0.01\n",
      "219.0\n",
      "0.01\n",
      "120.0\n",
      "0.01\n",
      "377.0\n",
      "0.01\n",
      "153.0\n",
      "0.01\n",
      "207.0\n",
      "0.01\n",
      "222.0\n",
      "0.01\n",
      "134.0\n",
      "0.01\n",
      "296.0\n",
      "0.01\n",
      "166.0\n",
      "0.01\n",
      "212.0\n",
      "0.01\n",
      "163.0\n",
      "0.01\n",
      "113.0\n",
      "0.01\n",
      "283.0\n",
      "0.01\n",
      "247.0\n",
      "0.01\n",
      "269.0\n",
      "0.01\n",
      "110.0\n",
      "0.01\n",
      "154.0\n",
      "0.01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-59e0e5e51955>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperience_replay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mtotReward\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-59e0e5e51955>\u001b[0m in \u001b[0;36mexperience_replay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0mytrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mytrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexploration\u001b[0m\u001b[0;34m*=\u001b[0m\u001b[0mDQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexploration_decay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexploration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexploration_min\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexploration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0mindex_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_shuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m                 \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_train_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.shuffle\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/numpy/core/_internal.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, array, ptr)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_ctypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mptr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import gym\n",
    "import numpy as np\n",
    "from random import sample\n",
    "\n",
    "##DEEP-Q-LEARNING, WE NEED TO USE EXPERIENCE REPLAY TO MAKE THIS WORK. SAVE EVERY EXPERIENCE IN A LIST\n",
    "##AND EVERYTIME WE TRAIN WE DRAW A RANDOM SAMPLE THAT WE TRAIN ON. IF WE ONLY TRAIN LATEST EXPERIENCE\n",
    "## WE WILL FORGET OLDER ONES WHICH ARE STILL IMPORTANT\n",
    "## REMEMBER ALSO THAT WE NEED TO TRAIN ON (S,A) JUST BEFORE TERMINATION SO CAN DO RIGHT MOVE IN \n",
    "## CRITICAL SITUATION\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "iterations = 100000\n",
    "#print(env.action_space) #vänster eller höger som motsvarar 0 eller 1\n",
    "#print(env.observation_space)#en state är en vektor längd 4. Position,hastighet,vinkel,vinkelhastighet\n",
    "   \n",
    "class DQN:\n",
    "    gamma = 0.95\n",
    "    exploration_decay=0.995\n",
    "    exploration_max=1.0\n",
    "    exploration_min=0.01\n",
    "    learning_rate=0.001\n",
    "    \n",
    "    def __init__(self,):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(24,input_shape=(4,),activation=\"relu\")) #tar väl egentligen in hela matrisen på en gång\n",
    "        self.model.add(Dense(24,activation=\"relu\"))\n",
    "        self.model.add(Dense(2,activation=\"linear\"))\n",
    "        self.model.compile(loss=\"mse\", optimizer=Adam(lr=DQN.learning_rate)) #sparar nu allting!\n",
    "        self.exploration=DQN.exploration_max\n",
    "        self.memory=[]\n",
    "        \n",
    "        \n",
    "    def choose_action(self,s):\n",
    "        q_vals = self.model.predict(s)[0] #här får en matris eftersom tanker är att kan predicta många samtidigt\n",
    "        if np.random.rand(1) <= 1-self.exploration:\n",
    "            return np.argmax(q_vals)\n",
    "        return 0 if np.random.rand(1) <= 1/2 else 1\n",
    "\n",
    "    def remember(self,state,action,reward,next_state,done):\n",
    "        self.memory.append((state,action,reward,next_state,done))\n",
    "    \n",
    "    def experience_replay(self):\n",
    "        if len(self.memory) < 25:\n",
    "            return\n",
    "        batch = sample(self.memory,24)\n",
    "        batch.append(dqn.memory[-1])\n",
    "        for state,action,reward,next_state,done in batch:\n",
    "            ytrain = self.model.predict(state)\n",
    "            if not done:\n",
    "                ytrain[0][action] = reward+DQN.gamma*np.max(self.model.predict(next_state)[0])\n",
    "            else:\n",
    "                ytrain[0][action] = reward\n",
    "            self.model.fit(x=state,y=ytrain,verbose=0)\n",
    "        self.exploration*=DQN.exploration_decay\n",
    "        self.exploration = np.maximum(DQN.exploration_min,self.exploration)\n",
    "\n",
    "        \n",
    "dqn = DQN()\n",
    "for _ in range(iterations):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state,(1,4)) #gör om vektor till matris. Varför? NN förväntar sig matris med rader som en train data\n",
    "    done = False\n",
    "    totReward=0\n",
    "    while not done:\n",
    "        action = dqn.choose_action(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        reward = reward if not done else -reward\n",
    "        next_state = np.reshape(next_state,(1,4))\n",
    "        dqn.remember(state,action,reward,next_state,done)\n",
    "        dqn.experience_replay()\n",
    "        totReward+=reward\n",
    "        state = next_state\n",
    "    print(totReward)\n",
    "    print(dqn.exploration)\n",
    "         \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dqn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-41fc57f1522a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dqn' is not defined"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    state = np.reshape(state,(1,4))\n",
    "    action = dqn.choose_action(state)\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    state = next_state\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7146/7146 [==============================] - 1s 77us/step - loss: 0.2337\n",
      "Epoch 2/10\n",
      "7146/7146 [==============================] - 0s 46us/step - loss: 0.2291\n",
      "Epoch 3/10\n",
      "7146/7146 [==============================] - 0s 41us/step - loss: 0.2282\n",
      "Epoch 4/10\n",
      "7146/7146 [==============================] - 0s 42us/step - loss: 0.2278\n",
      "Epoch 5/10\n",
      "7146/7146 [==============================] - 0s 58us/step - loss: 0.2271\n",
      "Epoch 6/10\n",
      "7146/7146 [==============================] - 0s 49us/step - loss: 0.2266\n",
      "Epoch 7/10\n",
      "7146/7146 [==============================] - 0s 41us/step - loss: 0.2266\n",
      "Epoch 8/10\n",
      "7146/7146 [==============================] - 0s 43us/step - loss: 0.2263\n",
      "Epoch 9/10\n",
      "7146/7146 [==============================] - 0s 45us/step - loss: 0.2259\n",
      "Epoch 10/10\n",
      "7146/7146 [==============================] - 0s 44us/step - loss: 0.2257\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11e908470>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "trainingdata = []\n",
    "for i in range(120000):\n",
    "    data = []\n",
    "    totReward = 0\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = 0 if np.random.rand(1) <= 1/2 else 1\n",
    "        data.append((state,action))\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        totReward += reward\n",
    "        state=next_state\n",
    "    if totReward >= 100:\n",
    "        for (state,action) in data:\n",
    "            trainingdata.append([state,action])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128,input_shape=(4,),activation=\"relu\"))\n",
    "model.add(Dense(52,activation=\"relu\"))\n",
    "model.add(Dense(1,activation=\"sigmoid\"))\n",
    "model.compile(loss=\"mse\",optimizer=\"adam\")\n",
    "\n",
    "xtrain=np.zeros(shape=(len(trainingdata),4))\n",
    "ytrain = np.zeros(shape=(len(trainingdata),1))\n",
    "for i in range(len(trainingdata)):\n",
    "    xtrain[i] = trainingdata[i][0]\n",
    "    ytrain[i] = trainingdata[i][1]\n",
    "\n",
    "\n",
    "model.fit(x=xtrain,y=ytrain,epochs=10)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-dc2a17fdc477>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "state = env.reset()\n",
    "done = False\n",
    "totReward = 0\n",
    "while not done:\n",
    "    state = np.reshape(state,(1,4))\n",
    "    action = model.predict(state)[0]\n",
    "    action = 0 if action < 1/2 else 1\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    state = next_state\n",
    "    totReward += reward\n",
    "print(totReward)\n",
    "\n",
    "##turns out to give better results than the DQN..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "state = env.reset()\n",
    "done = False\n",
    "totReward = 0\n",
    "while not done:\n",
    "    action = np.random.rand(1)\n",
    "    action = 0 if action < 1/2 else 1\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
