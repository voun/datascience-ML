{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• The game is played with an infinite deck of cards (i.e. cards are sampled\n",
    "with replacement)\n",
    "• Each draw from the deck results in a value between 1 and 10 (uniformly\n",
    "distributed) with a colour of red (probability 1/3) or black (probability\n",
    "2/3).\n",
    "• There are no aces or picture (face) cards in this game\n",
    "• At the start of the game both the player and the dealer draw one black\n",
    "card (fully observed)\n",
    "• Each turn the player may either stick or hit\n",
    "• If the player hits then she draws another card from the deck\n",
    "• If the player sticks she receives no further cards\n",
    "• The values of the player’s cards are added (black cards) or subtracted (red\n",
    "cards)\n",
    "• If the player’s sum exceeds 21, or becomes less than 1, then she “goes\n",
    "bust” and loses the game (reward -1)\n",
    "• If the player sticks then the dealer starts taking turns. The dealer always\n",
    "sticks on any sum of 17 or greater, and hits otherwise. If the dealer goes\n",
    "bust, then the player wins; otherwise, the outcome – win (reward +1),\n",
    "lose (reward -1), or draw (reward 0) – is the player with the largest sum.\n",
    "\n",
    "\n",
    "\n",
    "You should write an environment that implements the game Easy21. Specifically, write a function, named step, which takes as input a state s (dealer’s first\n",
    "card 1–10 and the player’s sum 1–21), and an action a (hit or stick), and returns\n",
    "a sample of the next state s\n",
    "0\n",
    "(which may be terminal if the game is finished)\n",
    "and reward r. We will be using this environment for model-free reinforcement\n",
    "learning, and you should not explicitly represent the transition matrix for the\n",
    "MDP. There is no discounting (γ = 1). You should treat the dealer’s moves as\n",
    "part of the environment, i.e. calling step with a stick action will play out the\n",
    "dealer’s cards and return the final reward and terminal state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(s,a): ## returnar nästa state som är \"terminal\" eller en tuple och reward\n",
    "    dCard = s[0]\n",
    "    mySum = s[1]\n",
    "    if a == \"hit\":\n",
    "        p = np.random.uniform(0,1)\n",
    "        mySum += np.random.randint(1,11)*(-1 if p <=1/3 else 1)\n",
    "        if mySum > 21 or mySum < 1:\n",
    "            return (\"terminal\",-1)\n",
    "        else:\n",
    "            return ((dCard,mySum),0)    \n",
    "    else:\n",
    "        dSum = dCard\n",
    "        while True:\n",
    "            if dSum < 1:\n",
    "                return (\"terminal\",1)\n",
    "            elif dSum >= 1 and dSum < 17: #dSum < mySum: \n",
    "                p = np.random.uniform(0,1)\n",
    "                dSum += np.random.randint(1,11)*(-1 if p <=1/3 else 1)\n",
    "            else:\n",
    "                break\n",
    "        if dSum > 21 or mySum > dSum:\n",
    "            return (\"terminal\",1)\n",
    "        elif mySum < dSum:\n",
    "            return (\"terminal\",-1)\n",
    "        else:\n",
    "            return (\"terminal\",0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 15)\n",
      "0\n",
      "('terminal', -1)\n"
     ]
    }
   ],
   "source": [
    "s = ((5,7))\n",
    "a = \"hit\"\n",
    "newS,rew = step(s,a)\n",
    "print(newS)\n",
    "print(rew)\n",
    "\n",
    "print(step((newS),\"stick\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Apply Monte-Carlo control to Easy21. Initialise the value function to zero. Use\n",
    "a time-varying scalar step-size of αt = 1/N(st, at) and an eps-greedy exploration\n",
    "strategy with eps = N0/(N0 + N(st)), where N0 = 100 is a constant, N(s) is\n",
    "the number of times that state s has been visited, and N(s, a) is the number\n",
    "of times that action a has been selected from state s. Feel free to choose an\n",
    "alternative value for N0, if it helps producing better results. Plot the optimal\n",
    "value function V\n",
    "∗\n",
    "(s) = maxa Q∗\n",
    "(s, a) using similar axes to the following figure\n",
    "taken from Sutton and Barto’s Blackjack example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: 0.1189634864546526\n",
      "3: 0.11641620696214558\n",
      "4: 0.11750269951899495\n",
      "5: 0.09351221465758935\n",
      "6: 0.1071041481919397\n",
      "7: 0.1070240657405595\n",
      "8: 0.1099455714992577\n",
      "9: 0.0970911192448475\n",
      "10: 0.14348061231018586\n",
      "11: 0.14538813789326566\n",
      "12: 0.10298102981029818\n",
      "13: 0.14208389715832198\n",
      "14: 0.06277372262773724\n",
      "15: 0.08069164265129687\n",
      "16: 0.10668563300142242\n",
      "17: 0.1880877742946709\n",
      "18: 0.4649776453055141\n",
      "19: 0.6222222222222219\n",
      "20: 0.7722473604826542\n",
      "21: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def helper1 (s,actionsInState,N): \n",
    "    actions = actionsInState[s]\n",
    "    sum = 0\n",
    "    for a in actions:\n",
    "        sum += N[(s,a)]\n",
    "    return sum\n",
    "\n",
    "def helper2(s,actionsInState,Q):\n",
    "    if len(actionsInState[s]) == 0:\n",
    "        p = np.random.uniform(0,1)\n",
    "        return \"hit\" if p <= 1/2 else \"stick\"\n",
    "    bestAction = None\n",
    "    bestValue = -1000000\n",
    "    actions = actionsInState[s] ##check here before if Q empty or something\n",
    "    for a in actions:\n",
    "        if Q[(s,a)] > bestValue:\n",
    "            bestAction = a\n",
    "            bestValue = Q[(s,a)]\n",
    "    return bestAction\n",
    "    \n",
    "\n",
    "N0 = 100\n",
    "Q = {} ##maps (s,a) --> Q(s,a)\n",
    "actionsInState = {} ##maps s --> list of actions done in this state\n",
    "N = {} ##maps (s,a) --> antal ggr har gjort action a i state s (dvs paret (s,a))\n",
    "samples = 1000000\n",
    "V  = {}\n",
    "\n",
    "for _ in range(samples):\n",
    "    returns = {} #maps (s,a) --> return for this sample from (s,a) pair\n",
    "    s = (np.random.randint(1,11),np.random.randint(1,11))\n",
    "    done = False\n",
    "    while not done:\n",
    "         \n",
    "        if not s in actionsInState:\n",
    "            actionsInState[s] = []\n",
    "        eps = N0/(N0+helper1(s,actionsInState,N)) ##fel, uppdatera efter varje sample\n",
    "        a = None                                ## men skitsamma\n",
    "        if np.random.uniform(0,1) < 1-eps:\n",
    "            a = helper2(s,actionsInState,Q)\n",
    "        else:\n",
    "            p = np.random.uniform(0,1)\n",
    "            a = \"hit\" if p <= 1/2 else \"stick\"\n",
    "        \n",
    "         \n",
    "        if not a in actionsInState[s]:\n",
    "            actionsInState[s].append(a)   \n",
    "        if not (s,a) in returns:\n",
    "            returns[(s,a)] = 0\n",
    "        if not (s,a) in N:\n",
    "            N[(s,a)] = 0\n",
    "        N[(s,a)] += 1\n",
    "        if not (s,a) in Q:\n",
    "            Q[(s,a)]= 0\n",
    "           \n",
    "        (news,rew) = step(s,a)\n",
    "        for (state,action) in returns:\n",
    "            returns[(state,action)] += rew\n",
    "        if news == \"terminal\":\n",
    "            done = True\n",
    "        else:\n",
    "            s = news\n",
    "    ##now here we should update the Q values\n",
    "    for (s,a) in returns:\n",
    "        Q[(s,a)] += 1/N[(s,a)]*(returns[(s,a)]-Q[(s,a)])\n",
    "\n",
    "for s in actionsInState:\n",
    "    V[s] = Q[(s,helper2(s,actionsInState,Q))]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "for s in range(2,22):\n",
    "    print(str(s) + \": \" + str(V[(5,s)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4599\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sum = 0\n",
    "for i in range(0,100000):\n",
    "    s = (np.random.randint(1,11),np.random.randint(1,11))\n",
    "    while True:\n",
    "        action = helper2(s,actionsInState,Q)\n",
    "        (newS,rew) = step(s,action)\n",
    "        if newS == \"terminal\":\n",
    "            sum += rew\n",
    "            break\n",
    "        s = newS\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
